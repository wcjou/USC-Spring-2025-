{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Decision Tree Pruning with Cost-Complexity Alpha (`ccp_alpha`)\n",
    "This notebook walks through how to prune a Decision Tree Classifier using cost-complexity pruning in `scikit-learn`. We'll:\n",
    "- Train a decision tree\n",
    "- Get the pruning path\n",
    "- Cross-validate to find the best `ccp_alpha`\n",
    "- Plot the results\n",
    "- Train the final tree using the best `ccp_alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries and generate example data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate cost-complexity pruning path\n",
    "# ----------------------------------------------------\n",
    "# Create a blank Decision Tree Classifier.\n",
    "# Setting random_state=0 ensures consistent results across runs.\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Generate the pruning path using the training data.\n",
    "# This gives us a series of possible ccp_alpha values and corresponding tree impurities.\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "# Extract the list of ccp_alpha values from the pruning path.\n",
    "# Each value represents a different level of pruning severity.\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Extract the total impurity at each level of pruning.\n",
    "# This shows how the impurity changes as the tree is simplified.\n",
    "impurities = path.impurities\n",
    "\n",
    "# Display the alpha values to inspect pruning options\n",
    "print(\"Available ccp_alpha values:\", ccp_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Cross-validate to find the best alpha\n",
    "# ----------------------------------------------------\n",
    "# We loop through each alpha value from the pruning path,\n",
    "# train a decision tree using that alpha, and perform 5-fold cross-validation.\n",
    "# We store the mean CV accuracy and the model for each alpha.\n",
    "clfs = []\n",
    "alpha_scores = []\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    clfs.append(clf)\n",
    "    alpha_scores.append(np.mean(scores))\n",
    "\n",
    "# Display alpha and scores\n",
    "for a, s in zip(ccp_alphas, alpha_scores):\n",
    "    print(f\"Alpha: {a:.5f}  |  CV Accuracy: {s:.4f}\")\n",
    "\n",
    "# ðŸ§  Pick the alpha that gives the highest CV accuracy\n",
    "best_alpha_index = np.argmax(alpha_scores)  # index of highest accuracy\n",
    "best_alpha = ccp_alphas[best_alpha_index]   # corresponding best alpha\n",
    "print(f\"\\nâœ… Best alpha based on cross-validation: {best_alpha:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot accuracy vs alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, alpha_scores, marker='o', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Mean Cross-Validation Accuracy\")\n",
    "plt.title(\"Cross-Validation Accuracy vs CCP Alpha\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train final model with best alpha\n",
    "# ---------------------------------------------\n",
    "# Now that we've found the best alpha from cross-validation,\n",
    "# we train a final decision tree using that alpha.\n",
    "final_clf = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha)\n",
    "final_clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.5f}\")\n",
    "print(f\"Final model accuracy on test set: {final_clf.score(X_test, y_test):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

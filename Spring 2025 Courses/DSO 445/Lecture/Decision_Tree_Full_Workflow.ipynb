{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd3b288",
   "metadata": {},
   "source": [
    "### Step 1: Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79066e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example dataset\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)  # X = features, y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27936e10",
   "metadata": {},
   "source": [
    "### Step 2: Split into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d92483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to train the model and keep some data for unbiased testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6de16",
   "metadata": {},
   "source": [
    "### Step 3: Fit full (unpruned) decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83291e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a full tree (i.e., grow it completely) before any pruning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbcd98",
   "metadata": {},
   "source": [
    "### Step 4: Get cost-complexity pruning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74dd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all `ccp_alpha` values where the tree structure changes — used for pruning\n",
    "path = tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa704911",
   "metadata": {},
   "source": [
    "### Step 5: Train a tree for each ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple pruned trees to evaluate how `ccp_alpha` affects performance\n",
    "trees = []\n",
    "for alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    trees.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34c605",
   "metadata": {},
   "source": [
    "### Step 6: Plot accuracy vs ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how pruning impacts overfitting vs underfitting\n",
    "import matplotlib.pyplot as plt\n",
    "train_scores = [clf.score(X_train, y_train) for clf in trees]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in trees]\n",
    "\n",
    "plt.plot(ccp_alphas, train_scores, label='Train')\n",
    "plt.plot(ccp_alphas, test_scores, label='Test')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Pruning Level')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995a8ec",
   "metadata": {},
   "source": [
    "### Step 7: Select best ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the `ccp_alpha` value that yields the best validation performance\n",
    "import numpy as np\n",
    "best_index = np.argmax(test_scores)\n",
    "best_alpha = ccp_alphas[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dab2f8",
   "metadata": {},
   "source": [
    "### Step 8: Train final pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best pruning level to train a tree that generalizes well\n",
    "final_tree = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha)\n",
    "final_tree.fit(X_train, y_train)\n",
    "print(\"Final accuracy:\", final_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05595e74",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Use pruned tree in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9562956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, RF trees are not pruned, but you can try this if needed\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "base_tree = DecisionTreeClassifier(ccp_alpha=best_alpha, random_state=0)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffd71a",
   "metadata": {},
   "source": [
    "### Step 10: Tune n_estimators with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CV to find the number of trees that performs best — no overfitting here\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for n in [50, 100, 150, 200]:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=0)\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "    print(f\"n={n} → Mean CV Accuracy: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbe938",
   "metadata": {},
   "source": [
    "### Step 11: Final model evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test on unseen data for true generalization performance\n",
    "best_rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "best_rf.fit(X_train, y_train)\n",
    "print(\"Test accuracy:\", best_rf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

# -*- coding: utf-8 -*-
"""Lecture6_LogReg_KNN_Iris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mC6ge0FOfMhFQskJTyvIXbjX-Ho148b7
"""



"""# **Part I: Logistic Regression for Classification on Iris Dataset.**"""

### Necessary Imports ###
### Imports enable us to use functionality from outside packages, like Numpy, Pandas, Scikit-Learn ###

### Math / Data Packages
import numpy as np
import pandas as pd

### Packages for Plotting
import seaborn as sns
import matplotlib.pyplot as plt

### Scikit-Learn Packages
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

### Confusion Matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

### Reading the CSV ###

iris = pd.read_csv('sample_data/Iris.csv')
print(iris.head())

### Drop the ID that came with the file to use the Dataframe ID instead ###

tmp = iris.drop('Id', axis=1) # axis = 0 is by row, axis = 1 is by column
tmp.head()

### Optional: use the package Seaborn to creare useful analysis plots like Pairplot ###

sns.pairplot(tmp, hue='Species', markers='x')
plt.show()   # .show() is only neccessary for PyCharm, not Colab

iris.describe()   # works only in Colab

### STEP 1: Create Feature Matrix X and target vector y from Dataframe ###

X = iris.drop(['Id', 'Species'], axis=1)
y = iris['Species']
print(X.shape)
print(y.shape)

### STEP 2: Split X and y into a train sample and a test sample ###

# train_test_split randomly chooses a percentage of rows in the dataset to be training data, and the rest to be test data
# the train/test data is assigned to 4 variables:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5) # 30% used for the test sample
print(X_train.shape)
print(X_test.shape)

### STEP 3: Now we define and train the model ###

### STEP 3.1: Define the model. Call it anything you like.  ###
model = LogisticRegression()   # model definition

### STEP 3.2: fit the model to the training data ###
model.fit(X_train, y_train)   # train

### STEP 3.3: use the trained model to predict the target for the test data ###
y_pred = model.predict(X_test)  # evaluation

# when we print the prediction we see that it contains the predicted classes:
y_pred

### ACCESSING ALL CLASS PROBABILITIES ###

# We can also access the full array of probabilities the model calculated for the test dataset
# We see that we get 3 probabilities for each row of the test data (1 probability for each class of plant)
# As seen above, y_pred assigns the class with maximum probability as the model prediction

y_proba = model.predict_proba(X_test)   # get access to the real probabilites for each class
y_proba

### MODEL ACCURACY ###
# Now we need to compare the prediction on X_test (y_pred) with the REAL label y_test to get a
# sense for how good the model actually is.
# We access the accuracy using the function accuracy_score that's part of the package metrics.
# Sidenote: we can access functions in packages using a Dot(.) operator
metrics.accuracy_score(y_test, y_pred)

### CONFUSION MATRIX ###
# We create a confusion matrix (as a variable) using the metrics.consfusion_matrix function.
# Of course we need to feed in the predicted classes, and the true labels
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)

# You can do a simple print
print(cnf_matrix)

# Or a really nice graphic display. Note that you need to feed in the matrix variable from above
# in the first argument, and the class labels in the second. You can access them using the variable
# model.classes_ that is created when the model was trained.
ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=model.classes_).plot()

print(X_train.shape)
y_pred = model.predict([[120, 100, 80, 20]])
print(y_pred)

"""# **Part II: KNN Algorithm for Classification on Iris Dataset.**

# First Attempt: Train on the full data set (not recommended -> Overfitting)
"""

model_knn = KNeighborsClassifier(n_neighbors = 5)

# We keep all the variables from before (X_train, X_test etc).
# In this first attempt we train the model on the full data X and y.
# This can be dangerous as we have no idea if our model might be too complex as we cannot independently
# calculate accuray / error on a training and test sample.

# We start with a range of values for k, in this case from 1 to 26:
ks = list(range(1,26))

# An empty list for our results:
scores = []

# As in the example of kMeans we create a for loop to run the model for different values of k
for k in ks:
    model = KNeighborsClassifier(n_neighbors=k)   # define model
    model.fit(X, y)  # fit the model to THE FULL DATA SET
    y_pred = model.predict(X)   # predict
   # print(scores)
    scores.append(metrics.accuracy_score(y, y_pred))  # Compare true label y with prediction and append the results to the scores list

# Plot the results; simple plot of the scores against the range of k contained in ks variable
# plt.plot is a standard plotting function from the matplotlib package
plt.plot(ks, scores)
plt.xlabel('Value of k for KNN')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')
plt.show()

"""# Now we will split the Data"""

### Now we will use X_train and the other split variables from above ###

# We will use a little bit different procedure to run the loop and store the scores
# First define maximum k
ks = 50
# try ks = 100 ;)

# This statement creates evenly spaced values within a given interval (1,ks)
neighbors = np.arange(1,ks)
# empty lists to store the results:
train_accuracy = np.empty(ks-1)  # compare to [] from before
test_accuracy = np.empty(ks-1)   # compare to [] from before

# Same loop as before
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k)  # in every loop we have a new k
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)

    # store the scores for train and test accuracy in the results lists
    train_accuracy[k-1] = knn.score(X_train, y_train)
    test_accuracy[k-1] = knn.score(X_test,y_test)

# Now we plot the results.
#plt.figure(2) creates a plot with 2 components
plt.figure(2)
plt.title("KNN: Varying number of neighbors")
plt.plot(neighbors, test_accuracy, label="Test Accuracy")
plt.plot(neighbors, train_accuracy, label = "Train Accuracy")
plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")
plt.show()

"""Now lets use our model to make a singular prediction for a new datapoint that we manually feed in:"""

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)

# this is our new out-of-sample datapoint. It's a list within a list, because the model expects an array of data (think our original X or X_train)
y_pred = knn.predict([[12, 10, 8, 2]])
print(y_pred)
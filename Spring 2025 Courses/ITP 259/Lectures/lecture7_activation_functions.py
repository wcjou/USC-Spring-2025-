# -*- coding: utf-8 -*-
"""Lecture7_Activation_Functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9JjrkKV9Qg7xG1MqfiCvc2ZXHBqp7nY
"""

#importing the required libraries
from math import exp
import matplotlib.pyplot as plt

#defining the sigmoid function
def sigmoid(x):
    return 1/(1+exp(-x))

#defining the tanh function using the relation
def tanh(x):
    return 2*sigmoid(2*x)-1

#input to the tanh function
input = []
for x in range(-5, 5):
    input.append(x)

#output of the tanh function
output = []
for ip in input:
    output.append(tanh(ip))

#plotting the graph for tanh function
plt.plot(input, output)
plt.grid()
#adding labels to the axes
plt.title("tanh activation function")
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.show()

# more elgantly:

def linear(x):
    # returns y = x
    return x

def Step(x):
    # returns '0' is the input is less then zero, otherwise returns one
    return np.heaviside(x,1)

def tanh(x):
    # returns the value (1-exp(-2x))/(1+exp(-2x)), values lie between -1 to 1

    return np.tanh(x)

def sigmoid(x):
    # returns 1/(1+exp(-x)), values lie between zero and one

    return 1/(1+np.exp(-x))

def RELU(x):
    # returns zero if the input is less than zero, otherwise it returns the given input
    x1=[]
    for i in x:
        if i<0:
            x1.append(0)
        else:
            x1.append(i)

    return x1

def softmax(x):
    # returns softmax values for each value in x.
    return np.exp(x) / np.sum(np.exp(x), axis=0)

x = np.linspace(-4, 4)

plt.plot(x, linear(x), label="Linear")
plt.plot(x, Step(x), label="binaryStep")
plt.plot(x, tanh(x), label="tanh")
plt.plot(x, sigmoid(x), label="sigmoid")
plt.plot(x, RELU(x), label="RELU")
plt.plot(x, softmax(x), label="softmax")
plt.legend(loc="upper left")
plt.show()

x = np.linspace(-10, 10)
plt.plot(x, softmax(x), label="softmax")
plt.show()
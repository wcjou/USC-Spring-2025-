# -*- coding: utf-8 -*-
"""Lecture8_Spiral_TF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bfLrEz-YVJ3vb864ccO-hN_jHg7zGrYQ
"""

import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import RMSprop
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits import mplot3d
from sklearn.model_selection import train_test_split

# create a model
def create_model():
    model = tf.keras.Sequential()
    # Input layer
    model.add(tf.keras.layers.Dense(12, input_dim=2, activation='relu'))
    model.add(tf.keras.layers.Dense(12,activation='relu'))
    model.add(tf.keras.layers.Dense(12,activation='relu'))
    # Output layer
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

    # Compile a model
    model.compile(loss='binary_crossentropy',
                optimizer=tf.keras.optimizers.Adam(learning_rate),
                metrics=['accuracy'])
    return model

def spirals(N, noise=.5):
    # create an array of values for theta randmoly distributed between 0 and 2pi
    theta = np.random.rand(N) * 4 * np.pi
    # r_a = const gives us a spherical distribution:
    # r_a = 1
    # with r depending on theta we create the Spiral, i.e. outwardly increasing radius
    r_a = 2*theta + np.pi
    # The transpose transforms the (2,400) array into a (400,2) array:
    data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T
    #add random noise
    x_a = data_a + np.random.randn(N,2)
    # r_b = -1
    r_b = -2*theta - np.pi
    data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T
    x_b = data_b + np.random.randn(N,2)
    # append a column to control color in the scatter plot
    res_a = np.append(x_a, np.zeros((N,1)), axis=1)
    res_b = np.append(x_b, np.ones((N,1)), axis=1)
    # combine the two spiral arrays into one with 800 rows and 3 columns
    res = np.append(res_a, res_b, axis=0)
    #np.random.shuffle(res)
    X = res[:,0:2]
    y = res[:,2]
    return (X,y)

X, y = spirals(1000)

# Lets create the train and test data set
np.random.seed(4375689)

X_train, X_test , Y_train, Y_test = \
    train_test_split(X, y, test_size=0.30, random_state=2023, stratify=y)

plt.figure(figsize=(12,8))
plt.scatter(X_train[:,0],X_train[:,1], color = 'b', label = 'class X_train')
plt.scatter(X_test[:,0],X_test[:,1], color = 'r', label = 'class X_test')
plt.xlabel('feature1')
plt.ylabel('feature2')
plt.legend()
plt.axis('equal')
plt.show()

print('X_train:\t{}' .format(X_train.shape))
print('Y_train:\t{}' .format(Y_train.shape))
print('X_test:\t\t{}'.format(X_test.shape))
print('Y_test:\t\t{}'.format(Y_test.shape))

# Hyperparameters
training_epochs = 1000 # Total number of training epochs
learning_rate = 0.001 # The learning rate

model = create_model()
model.summary()

results = model.fit(
 X_train, Y_train.T,
 epochs= training_epochs,
 validation_data = (X_test, Y_test.T),
 verbose = 0
)

prediction_values = model.predict(X_test)
classes_values = np.argmax(prediction_values,axis=1)

print(np.mean(results.history["val_accuracy"]))

print("Evaluating on training set...")
(loss, accuracy) = model.evaluate(X_train, Y_train.T, verbose=0)
print("loss={:.4f}, accuracy: {:.4f}%".format(loss,accuracy * 100))

print("Evaluating on testing set...")
(loss, accuracy) = model.evaluate(X_test, Y_test.T, verbose=0)
print("loss={:.4f}, accuracy: {:.4f}%".format(loss,accuracy * 100))

# summarize history for accuracy
plt.plot(results.history['accuracy'])
plt.plot(results.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='best')
plt.show()

# summarize history for loss
plt.plot(results.history['loss'])
plt.plot(results.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

max_loss = np.max(results.history['loss'])
min_loss = np.min(results.history['loss'])
print("Maximum Loss : {:.4f}".format(max_loss))
print("")
print("Minimum Loss : {:.4f}".format(min_loss))
print("")
print("Loss difference : {:.4f}".format((max_loss - min_loss)))


# Plot decision boundary
xx = np.linspace(-40, 40, 400)
yy = np.linspace(-40, 40, 400)
gx, gy = np.meshgrid(xx, yy)
Z = model.predict(np.c_[gx.ravel(), gy.ravel()])
Z = Z.reshape(gx.shape)
plt.contourf(gx, gy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

axes = plt.gca()
axes.set_xlim([-40, 40])
axes.set_ylim([-40, 40])
plt.grid('off')
plt.axis('off')

plt.scatter(X_test[:,0], X_test[:,1], c=prediction_values[:,0], cmap=cm.coolwarm)
plt.title('Model predictions on our Test set')
plt.show()
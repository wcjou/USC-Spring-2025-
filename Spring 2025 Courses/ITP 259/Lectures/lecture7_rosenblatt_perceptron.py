# -*- coding: utf-8 -*-
"""Lecture7_Rosenblatt_Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-hnKfn-piZs2xztNsSn-dmNHGhXQDk68
"""

# Credit: https://github.com/christianversloot/machine-learning-articles/blob/main/linking-maths-and-intuition-rosenblatts-perceptron-in-python.md?plain=1

import numpy as np

## Prepare a dataset

# Generate target classes {0, 1}
zeros = np.zeros(50)
ones = zeros + 1
targets = np.concatenate((zeros, ones))

# print(zeros)
# print(ones)
# print(targets)

# Generate data
# normal has attributes mu, sigma, output shape
small = np.random.normal(5, 0.25, (50,2))
large = np.random.normal(6.5, 0.25, (50,2))

# print(small)
# print(large)

# Prepare input data
X = np.concatenate((small,large))
T = targets

### Visualizing the dataset

from mlxtend.plotting import plot_decision_regions
import matplotlib.pyplot as plt
plt.scatter(small[:,0], small[:,1], color='blue')
plt.scatter(large[:,0], large[:,1], color='red')
plt.show()

# Rosenblatt Perceptron

import numpy as np

# Basic Rosenblatt Perceptron implementation
class RBPerceptron:

  # Constructor object, self is the instance of the object RBPerceptron itself
  def __init__(self, number_of_epochs = 100, learning_rate = 0.1):
    self.number_of_epochs = number_of_epochs
    self.learning_rate = learning_rate

  # Train perceptron
  def train(self, X, T):
    # Initialize weights vector with zeroes
    num_features = X.shape[1]
    self.w = np.zeros(num_features + 1)
    # Perform the epochs
#    diff = []
#    err = []
    for i in range(self.number_of_epochs):
#      err.append(sum(diff))
#      print(err)
      # For every combination of (X_i, T_i), zip creates the tuples
      for sample, desired_outcome in zip(X, T):
        # Generate prediction and compare with desired outcome
        prediction    = self.predict(sample)
        difference    = (desired_outcome - prediction)
#        diff.append(difference)
        # Compute weight update via Perceptron Learning Rule
        self.w[1:]    += self.learning_rate * difference * sample
        self.w[0]     += self.learning_rate * difference
    return self

  # Generate prediction
  def predict(self, sample):
    # dot product:
    outcome = np.dot(sample, self.w[1:]) + self.w[0]
    # Activation function:
    return np.where(outcome > 0, 1, 0)

# ks = [100,200,300,400,500,600,700, 800, 900, 1000]
ks = [100,200, 300, 500]
colors = ['blue','limegreen','gray','cyan','red','red','red']

for k in ks:
  # print(k)
  rbp = RBPerceptron(k, 0.1)
  trained_model = rbp.train(X, T)

  plot_decision_regions(X, T.astype(np.integer), clf=trained_model, legend=0)

plt.title('Perceptron')
plt.xlabel('X1')
plt.ylabel('X2')
plt.show()
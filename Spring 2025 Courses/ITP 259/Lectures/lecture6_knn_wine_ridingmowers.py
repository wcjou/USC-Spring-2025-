# -*- coding: utf-8 -*-
"""Lecture6_KNN_Wine_ridingmowers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UMDZKIyPSLvtr1c-jDfVKh8X8ne9N6Z

# Example 1: Sales of Lawn Mowers
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from sklearn import  metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

mowers = pd.read_csv("sample_data/RidingMowers.csv")

mowers

from sklearn.preprocessing import Normalizer
normalizer = Normalizer()
# exclude the 'person' column:
X = mowers.iloc[:,1:3]
y = mowers.iloc[:,3]
normalizer.fit(X)
X = pd.DataFrame(normalizer.transform(X), columns=X.columns)
normalizer.fit(X)
print(X)
print(y)

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from sklearn import  metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

mowers = pd.read_csv("sample_data/RidingMowers.csv")

mowers.drop(mowers.columns[0], axis=1, inplace=True)
#print(mowers)
X = mowers.iloc[:,0:2]
y = mowers.iloc[:,2]

#print(X)
#print(y)

##### Pre-Processing #####

# 1. Normalization
from sklearn.preprocessing import Normalizer
normalizer = Normalizer()
normalizer.fit(X)
X = pd.DataFrame(normalizer.transform(X), columns=X.columns)
# print(X)

# 2. Split data into train and test
X_train, X_test, y_train, y_test = \
    train_test_split(X,y, test_size=0.3, random_state=2022, stratify=y)

print(X_train)
print(X_test)
 
neighbors = np.arange(1,25)
train_accuracy = np.empty(24)
test_accuracy = np.empty(24)

for k in neighbors:
    print(k)
    # Modeling Step 1: Define the model
    knn = KNeighborsClassifier(n_neighbors=k)
    # Modeling Step 2: fitting on training data
    knn.fit(X_train, y_train)
    # Modeling Step 3: Evaluation on test data
    y_pred = knn.predict(X_test)
    # Step 4: Visualization
    cf = metrics.confusion_matrix(y_test, y_pred)
    ConfusionMatrixDisplay(confusion_matrix=cf, display_labels=knn.classes_).plot()
    plt.show()
    # score returns the fraction of correctly classified samples
    train_accuracy[k-1] = knn.score(X_train, y_train)
    test_accuracy[k-1] = knn.score(X_test,y_test)
#    CV_accuracy[k-1] = np.mean(cross_val_score(knn, X, y, cv=5))
    print(test_accuracy[k-1])

plt.figure(3)
plt.title("KNN: Varying number of neighbors")
plt.plot(neighbors, test_accuracy, label="Test Accuracy")
plt.plot(neighbors, train_accuracy, label = "Train Accuracy")
# plt.plot(neighbors, CV_accuracy, label = "CV Accuracy")
plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")
plt.show()

"""# Example 2: Wine"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from sklearn import  metrics
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

wineData = pd.read_csv("sample_data/wineQualityReds.csv")

#print(wineData)

plt.hist(wineData["quality"])
plt.show()

#drop the wine bottle number
wineData.drop(wineData.columns[0], axis=1, inplace=True)
X = wineData.iloc[:,0:11]
y = wineData.iloc[:,11]

print(X)
print(y)

from sklearn.preprocessing import Normalizer
normalizer = Normalizer()
normalizer.fit(X)
X = pd.DataFrame(normalizer.transform(X), columns=X.columns)
#print(X)

# Here we only do training (Training A) and test partition (Training B) here. In the Colab file we also do a 3rd set, a cross-validation set.
# In the Homework you will do
# Training A
# Training B
# Test

X_train, X_test, y_train, y_test = \
    train_test_split(X,y, test_size=0.3, random_state=2022, stratify=y)

neighbors = np.arange(1,41)
train_accuracy = np.empty(40)
test_accuracy = np.empty(40)
CV_accuracy = np.empty(40)

for k in neighbors:
#   print(k)
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    cf = metrics.confusion_matrix(y_test, y_pred)
    #print(k)
    #ConfusionMatrixDisplay(confusion_matrix=cf, display_labels=knn.classes_).plot()
    #plt.show()
    # score returns the fraction of correctly classified samples
    train_accuracy[k-1] = knn.score(X_train, y_train)
    test_accuracy[k-1] = knn.score(X_test,y_test)
    CV_accuracy[k-1] = np.mean(cross_val_score(knn, X, y, cv=5))

plt.figure(3)
plt.title("KNN: Varying number of neighbors")
plt.plot(neighbors, test_accuracy, label="Test Accuracy")
plt.plot(neighbors, train_accuracy, label = "Train Accuracy")
plt.plot(neighbors, CV_accuracy, label = "CV Accuracy")
plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")
plt.show()